{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abomhold/Catagorical-Regression-Analysis/blob/master/RF_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x73unFT9Wt_x"
      },
      "source": [
        "## Pull data sets from GitHub and combine\n",
        "\n",
        "By Austin B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c4tiuya7VPZu",
        "outputId": "f1601e17-6cba-41c4-f8a6-f2fc797ad066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "               Date Time  S4.5cm@1m  E9cm@1m  N13.5cm@1m  E4.5cm@2m  N9cm@2m  \\\n",
            "0    2022-08-21 00:01:51      25.18    26.25       24.62      25.62    26.12   \n",
            "1    2022-08-21 00:05:07      25.18    26.18       24.62      25.68    26.12   \n",
            "2    2022-08-21 00:08:23      25.12    26.18       24.62      25.62    26.12   \n",
            "3    2022-08-21 00:11:38      25.18    26.18       24.56      25.62    26.06   \n",
            "4    2022-08-21 00:14:54      25.12    26.18       24.62      25.62    26.06   \n",
            "...                  ...        ...      ...         ...        ...      ...   \n",
            "3374 2022-08-28 23:45:31      25.18    25.68       25.12      25.62    26.06   \n",
            "3375 2022-08-28 23:48:46      25.18    25.68       25.06      25.62    26.06   \n",
            "3376 2022-08-28 23:52:02      25.18    25.68       25.06      25.62    26.06   \n",
            "3377 2022-08-28 23:55:17      25.18    25.68       25.06      25.62    26.06   \n",
            "3378 2022-08-28 23:58:32      25.18    25.68       25.00      25.62    26.06   \n",
            "\n",
            "      W13.5cm@2m  N4.5cm@3m  W9cm@3m  S13.5cm@3m  W_Ext_Temp@3.5m  \n",
            "0          26.00      25.25    26.00       25.81            23.93  \n",
            "1          26.00      25.25    26.00       25.81            23.81  \n",
            "2          26.00      25.18    25.93       25.81            23.75  \n",
            "3          26.00      25.18    26.00       25.81            23.68  \n",
            "4          26.00      25.18    26.00       25.75            23.62  \n",
            "...          ...        ...      ...         ...              ...  \n",
            "3374       25.75      25.37    25.93       25.50            25.12  \n",
            "3375       25.75      25.43    25.93       25.50            25.12  \n",
            "3376       25.75      25.37    25.93       25.50            25.06  \n",
            "3377       25.75      25.37    25.87       25.50            25.00  \n",
            "3378       25.75      25.37    25.87       25.50            24.56  \n",
            "\n",
            "[3379 rows x 11 columns]\n",
            "                Date Time Temperature Source  Temperature\n",
            "0     2022-08-21 00:01:51          S4.5cm@1m        25.18\n",
            "1     2022-08-21 00:05:07          S4.5cm@1m        25.18\n",
            "2     2022-08-21 00:08:23          S4.5cm@1m        25.12\n",
            "3     2022-08-21 00:11:38          S4.5cm@1m        25.18\n",
            "4     2022-08-21 00:14:54          S4.5cm@1m        25.12\n",
            "...                   ...                ...          ...\n",
            "33785 2022-08-28 23:45:31    W_Ext_Temp@3.5m        25.12\n",
            "33786 2022-08-28 23:48:46    W_Ext_Temp@3.5m        25.12\n",
            "33787 2022-08-28 23:52:02    W_Ext_Temp@3.5m        25.06\n",
            "33788 2022-08-28 23:55:17    W_Ext_Temp@3.5m        25.00\n",
            "33789 2022-08-28 23:58:32    W_Ext_Temp@3.5m        24.56\n",
            "\n",
            "[33790 rows x 3 columns]\n",
            "                    Temperature Source  Temperature\n",
            "Date Time                                          \n",
            "2022-08-21 00:01:51          S4.5cm@1m        25.18\n",
            "2022-08-21 00:05:07          S4.5cm@1m        25.18\n",
            "2022-08-21 00:08:23          S4.5cm@1m        25.12\n",
            "2022-08-21 00:11:38          S4.5cm@1m        25.18\n",
            "2022-08-21 00:14:54          S4.5cm@1m        25.12\n",
            "...                                ...          ...\n",
            "2022-08-28 23:45:31    W_Ext_Temp@3.5m        25.12\n",
            "2022-08-28 23:48:46    W_Ext_Temp@3.5m        25.12\n",
            "2022-08-28 23:52:02    W_Ext_Temp@3.5m        25.06\n",
            "2022-08-28 23:55:17    W_Ext_Temp@3.5m        25.00\n",
            "2022-08-28 23:58:32    W_Ext_Temp@3.5m        24.56\n",
            "\n",
            "[33790 rows x 2 columns]\n",
            "                    Temperature Source  Temperature  \\\n",
            "Date Time                                             \n",
            "2022-08-21 00:01:51          S4.5cm@1m        25.18   \n",
            "2022-08-21 00:05:07          S4.5cm@1m        25.18   \n",
            "2022-08-21 00:08:23          S4.5cm@1m        25.12   \n",
            "2022-08-21 00:11:38          S4.5cm@1m        25.18   \n",
            "2022-08-21 00:14:54          S4.5cm@1m        25.12   \n",
            "...                                ...          ...   \n",
            "2022-08-28 23:45:31    W_Ext_Temp@3.5m        25.12   \n",
            "2022-08-28 23:48:46    W_Ext_Temp@3.5m        25.12   \n",
            "2022-08-28 23:52:02    W_Ext_Temp@3.5m        25.06   \n",
            "2022-08-28 23:55:17    W_Ext_Temp@3.5m        25.00   \n",
            "2022-08-28 23:58:32    W_Ext_Temp@3.5m        24.56   \n",
            "\n",
            "                     Anemometer;wind_speed;Avg (m/s)  \\\n",
            "Date Time                                              \n",
            "2022-08-21 00:01:51                           2.1758   \n",
            "2022-08-21 00:05:07                           2.0381   \n",
            "2022-08-21 00:08:23                           2.0381   \n",
            "2022-08-21 00:11:38                           2.0381   \n",
            "2022-08-21 00:14:54                           2.0381   \n",
            "...                                              ...   \n",
            "2022-08-28 23:45:31                           1.3129   \n",
            "2022-08-28 23:48:46                           1.3129   \n",
            "2022-08-28 23:52:02                           1.3129   \n",
            "2022-08-28 23:55:17                           1.3129   \n",
            "2022-08-28 23:58:32                           1.3129   \n",
            "\n",
            "                     Wind Vane TMR;wind_direction;Avg (°)  \\\n",
            "Date Time                                                   \n",
            "2022-08-21 00:01:51                              195.8894   \n",
            "2022-08-21 00:05:07                              195.6512   \n",
            "2022-08-21 00:08:23                              195.6512   \n",
            "2022-08-21 00:11:38                              195.6512   \n",
            "2022-08-21 00:14:54                              195.6512   \n",
            "...                                                   ...   \n",
            "2022-08-28 23:45:31                              160.5655   \n",
            "2022-08-28 23:48:46                              160.5655   \n",
            "2022-08-28 23:52:02                              160.5655   \n",
            "2022-08-28 23:55:17                              160.5655   \n",
            "2022-08-28 23:58:32                              160.5655   \n",
            "\n",
            "                     Hygro/Thermo;humidity;Avg (%)  \\\n",
            "Date Time                                            \n",
            "2022-08-21 00:01:51                        73.0005   \n",
            "2022-08-21 00:05:07                        73.2226   \n",
            "2022-08-21 00:08:23                        73.2226   \n",
            "2022-08-21 00:11:38                        73.2226   \n",
            "2022-08-21 00:14:54                        73.2226   \n",
            "...                                            ...   \n",
            "2022-08-28 23:45:31                        81.9440   \n",
            "2022-08-28 23:48:46                        81.9440   \n",
            "2022-08-28 23:52:02                        81.9440   \n",
            "2022-08-28 23:55:17                        81.9440   \n",
            "2022-08-28 23:58:32                        81.9440   \n",
            "\n",
            "                     Hygro/Thermo;temperature;Avg (°C)  \\\n",
            "Date Time                                                \n",
            "2022-08-21 00:01:51                            23.8425   \n",
            "2022-08-21 00:05:07                            23.5854   \n",
            "2022-08-21 00:08:23                            23.5854   \n",
            "2022-08-21 00:11:38                            23.5854   \n",
            "2022-08-21 00:14:54                            23.5854   \n",
            "...                                                ...   \n",
            "2022-08-28 23:45:31                            24.9494   \n",
            "2022-08-28 23:48:46                            24.9494   \n",
            "2022-08-28 23:52:02                            24.9494   \n",
            "2022-08-28 23:55:17                            24.9494   \n",
            "2022-08-28 23:58:32                            24.9494   \n",
            "\n",
            "                     Barometer;air_pressure;Avg (hPa)  \\\n",
            "Date Time                                               \n",
            "2022-08-21 00:01:51                         1010.4501   \n",
            "2022-08-21 00:05:07                         1010.4067   \n",
            "2022-08-21 00:08:23                         1010.4067   \n",
            "2022-08-21 00:11:38                         1010.4067   \n",
            "2022-08-21 00:14:54                         1010.4067   \n",
            "...                                               ...   \n",
            "2022-08-28 23:45:31                         1011.1220   \n",
            "2022-08-28 23:48:46                         1011.1220   \n",
            "2022-08-28 23:52:02                         1011.1220   \n",
            "2022-08-28 23:55:17                         1011.1220   \n",
            "2022-08-28 23:58:32                         1011.1220   \n",
            "\n",
            "                     DNI (Direct Normal Irradiance) Pyrheliometer;solar_DNI;Avg (W/m²)  \n",
            "Date Time                                                                               \n",
            "2022-08-21 00:01:51                                            13.2365                  \n",
            "2022-08-21 00:05:07                                            13.1546                  \n",
            "2022-08-21 00:08:23                                            13.1546                  \n",
            "2022-08-21 00:11:38                                            13.1546                  \n",
            "2022-08-21 00:14:54                                            13.1546                  \n",
            "...                                                                ...                  \n",
            "2022-08-28 23:45:31                                            13.4618                  \n",
            "2022-08-28 23:48:46                                            13.4618                  \n",
            "2022-08-28 23:52:02                                            13.4618                  \n",
            "2022-08-28 23:55:17                                            13.4618                  \n",
            "2022-08-28 23:58:32                                            13.4618                  \n",
            "\n",
            "[33790 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install --user openpyxl\n",
        "\n",
        "# Reorder tree data by temp with column name as new feature\n",
        "url1 = \"https://raw.githubusercontent.com/yajuna/linearRegression/master/Tree_Temp_Values_AUG21_to_AUG28_2022.xlsx\"\n",
        "tree = pd.read_excel(url1)\n",
        "print(tree)\n",
        "tree = tree.melt(id_vars=['Date Time'], var_name='Temperature Source', value_name='Temperature')\n",
        "print(tree)\n",
        "\n",
        "# Format the date time column and set as index\n",
        "tree['Date Time'] = pd.to_datetime(tree['Date Time'], format='%m/%d/%Y %H:%M')\n",
        "tree.set_index('Date Time', inplace=True)\n",
        "print(tree)\n",
        "\n",
        "# Set the date time as a pd.datetime column and to the index\n",
        "url2 = \"https://raw.githubusercontent.com/yajuna/linearRegression/master/Weather_Station_AUG21_to_AUG28_2022.xlsx\"\n",
        "weather = pd.read_excel(url2)\n",
        "\n",
        "# Set the date_time as a pandas datetime and the index\n",
        "weather['datetime'] = pd.to_datetime(weather['datetime'])\n",
        "weather.set_index('datetime', inplace=True)\n",
        "\n",
        "# Reindex weather to match the tree data frame\n",
        "weather = weather.reindex(tree.index, method='nearest')\n",
        "\n",
        "# Combine the data frames\n",
        "combined = pd.concat([tree, weather], axis=1)\n",
        "print(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rslvZ0u-XzO0"
      },
      "source": [
        "## Clean the data and output to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "quA_VsWLTC7F",
        "outputId": "b461610b-e8b9-49cf-d2aa-823f40037165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     temperature  direction  depth  height  wind_speed  \\\n",
            "date_time                                                                \n",
            "2022-08-21 00:01:51        25.18          0    4.5     1.0      2.1758   \n",
            "2022-08-21 00:05:07        25.18          0    4.5     1.0      2.0381   \n",
            "2022-08-21 00:08:23        25.12          0    4.5     1.0      2.0381   \n",
            "2022-08-21 00:11:38        25.18          0    4.5     1.0      2.0381   \n",
            "2022-08-21 00:14:54        25.12          0    4.5     1.0      2.0381   \n",
            "...                          ...        ...    ...     ...         ...   \n",
            "2022-08-28 23:45:31        25.12         90    0.0     3.5      1.3129   \n",
            "2022-08-28 23:48:46        25.12         90    0.0     3.5      1.3129   \n",
            "2022-08-28 23:52:02        25.06         90    0.0     3.5      1.3129   \n",
            "2022-08-28 23:55:17        25.00         90    0.0     3.5      1.3129   \n",
            "2022-08-28 23:58:32        24.56         90    0.0     3.5      1.3129   \n",
            "\n",
            "                     wind_direction  air_humidity  air_temperature  \\\n",
            "date_time                                                            \n",
            "2022-08-21 00:01:51        195.8894       73.0005          23.8425   \n",
            "2022-08-21 00:05:07        195.6512       73.2226          23.5854   \n",
            "2022-08-21 00:08:23        195.6512       73.2226          23.5854   \n",
            "2022-08-21 00:11:38        195.6512       73.2226          23.5854   \n",
            "2022-08-21 00:14:54        195.6512       73.2226          23.5854   \n",
            "...                             ...           ...              ...   \n",
            "2022-08-28 23:45:31        160.5655       81.9440          24.9494   \n",
            "2022-08-28 23:48:46        160.5655       81.9440          24.9494   \n",
            "2022-08-28 23:52:02        160.5655       81.9440          24.9494   \n",
            "2022-08-28 23:55:17        160.5655       81.9440          24.9494   \n",
            "2022-08-28 23:58:32        160.5655       81.9440          24.9494   \n",
            "\n",
            "                     air_pressure  solar_DNI  \n",
            "date_time                                     \n",
            "2022-08-21 00:01:51     1010.4501    13.2365  \n",
            "2022-08-21 00:05:07     1010.4067    13.1546  \n",
            "2022-08-21 00:08:23     1010.4067    13.1546  \n",
            "2022-08-21 00:11:38     1010.4067    13.1546  \n",
            "2022-08-21 00:14:54     1010.4067    13.1546  \n",
            "...                           ...        ...  \n",
            "2022-08-28 23:45:31     1011.1220    13.4618  \n",
            "2022-08-28 23:48:46     1011.1220    13.4618  \n",
            "2022-08-28 23:52:02     1011.1220    13.4618  \n",
            "2022-08-28 23:55:17     1011.1220    13.4618  \n",
            "2022-08-28 23:58:32     1011.1220    13.4618  \n",
            "\n",
            "[33790 rows x 10 columns]\n"
          ]
        }
      ],
      "source": [
        "# Combine the data frames (duplicate to support only running second half of script)\n",
        "combined = pd.concat([tree, weather], axis=1)\n",
        "\n",
        "# clean the temperature source column\n",
        "combined['Temperature Source'] = combined['Temperature Source'].apply(\n",
        "    lambda x: x.replace('@', ' ')\n",
        "               .replace('cm', '')\n",
        "               .replace(',', ' ')\n",
        "               .replace('m', '')\n",
        "               )\n",
        "\n",
        "# Replace S/N/E/W with degrees\n",
        "directionDict = {'S': 0, 'N': 180, 'E': 270, 'W': 90}\n",
        "combined['direction'] = combined['Temperature Source'].apply(\n",
        "    lambda x: directionDict.get(x.split(' ')[0][0])\n",
        "    ).astype(int)\n",
        "\n",
        "# Convert the column header to a depth\n",
        "combined['depth'] = combined['Temperature Source'].apply(\n",
        "    lambda x: x.split(' ')[0][1:] if not x.split(' ')[0][1:].endswith(\"_Ext_Tep\") else 0\n",
        "    ).astype(float)\n",
        "\n",
        "# Convert the column header to a height\n",
        "combined['height'] = combined['Temperature Source'].apply(\n",
        "    lambda x: x.split(' ')[1]\n",
        "    ).astype(float)\n",
        "\n",
        "# Reorder the columns and drop redundant columns\n",
        "combined = combined.drop('Temperature Source', axis=1)\n",
        "cols = combined.columns.tolist()\n",
        "cols = [cols[0]] + cols[-3:] + cols[1:7]\n",
        "combined = combined[cols]\n",
        "\n",
        "# rename columns\n",
        "combined.index.names = ['date_time']\n",
        "combined.columns = ['temperature', 'direction', 'depth', 'height', 'wind_speed',\n",
        "                    'wind_direction', 'air_humidity', 'air_temperature',\n",
        "                    'air_pressure', 'solar_DNI']\n",
        "\n",
        "# Create a space column for the three spatial dimensions\n",
        "# # Multiply all values to\n",
        "# combined['space'] = combined[['direction', 'depth', 'height']].apply(list, axis=1)\n",
        "\n",
        "\n",
        "print(combined)\n",
        "\n",
        "# Save the cleaned data\n",
        "combined.to_csv('./data.csv')\n",
        "\n",
        "# Read the cleaned data\n",
        "# with open('./data.csv') as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePAAYQ7qy2C3"
      },
      "source": [
        "## Random Forrest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCZB0luduMCa",
        "outputId": "e9a9ec02-9cce-47d5-cfd6-d8aaadb28a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33790\n",
            "33790\n",
            "[[-1.50000000e+00 -5.00000000e-01 -5.00000000e-01 ...  2.47805290e-01\n",
            "   2.70938862e-01 -4.10399729e-05]\n",
            " [ 5.00000000e-01  0.00000000e+00  0.00000000e+00 ... -6.07962859e-01\n",
            "   4.51172105e-02 -1.96791675e-03]\n",
            " [ 1.50000000e+00  0.00000000e+00 -5.00000000e-01 ...  9.47917839e-01\n",
            "   7.09447520e-01  1.60791511e+00]\n",
            " ...\n",
            " [-5.00000000e-01 -1.00000000e+00  7.50000000e-01 ... -9.17360720e-01\n",
            "   3.05218518e-01 -2.93085465e-03]\n",
            " [ 1.50000000e+00  0.00000000e+00 -5.00000000e-01 ...  2.25745639e-01\n",
            "   2.13806102e-01  3.48339282e-04]\n",
            " [-5.00000000e-01  0.00000000e+00  5.00000000e-01 ... -4.79150253e-01\n",
            "   4.93933325e-01 -2.86979518e-03]]\n",
            "date_time\n",
            "2022-08-24 18:51:16    25.00\n",
            "2022-08-27 06:19:32    24.75\n",
            "2022-08-21 09:43:13    26.75\n",
            "2022-08-24 18:12:12    26.81\n",
            "2022-08-27 04:34:49    25.12\n",
            "                       ...  \n",
            "2022-08-25 17:48:48    27.37\n",
            "2022-08-21 00:27:57    25.75\n",
            "2022-08-26 05:10:17    20.93\n",
            "2022-08-21 19:05:51    27.12\n",
            "2022-08-23 21:44:10    23.12\n",
            "Name: temperature, Length: 27032, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer\n",
        "\n",
        "# Load the cleaned data\n",
        "data = pd.read_csv('./data.csv', index_col='date_time')\n",
        "# Shuffle data with fixed seed\n",
        "\n",
        "############ DO NOT EDIT THE MODEL WITHOUT FIRST PERFORMING THIS STEP ##########\n",
        "# It is crucial to the validity of our research\n",
        "# Delete the first 5% of rows as a blind test sample\n",
        "# This is for humans and models\n",
        "data = data.sample(frac=1, random_state=42)\n",
        "print(len(data))\n",
        "# data = data.iloc[int(len(data) * 0.05):]\n",
        "print(len(data))\n",
        "################################################################################\n",
        "# Define the target variable (temperature) and features\n",
        "X = data.drop(['temperature'], axis=1)  # Drop target and unnecessary columns\n",
        "y = data['temperature']\n",
        "\n",
        "# Keep a copy of the feature names before scaling\n",
        "feature_names = X.columns # Store column names here\n",
        "\n",
        "# Add a nomalization step\n",
        "scaler = RobustScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.utils import resample\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class CustomExtraTreesRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 random_state=None, criterion='squared_error', bootstrap=False,\n",
        "                 max_features=1.0):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.random_state = random_state\n",
        "        self.criterion = criterion\n",
        "        self.bootstrap = bootstrap\n",
        "        self.max_features = max_features\n",
        "        self.estimators_ = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Input validation\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.random_state_ = check_random_state(self.random_state)\n",
        "\n",
        "        # Calculate max_features\n",
        "        n_features = X.shape[1]\n",
        "        if isinstance(self.max_features, float):\n",
        "            self.max_features_ = max(1, int(self.max_features * n_features))\n",
        "        elif isinstance(self.max_features, int):\n",
        "            self.max_features_ = self.max_features\n",
        "        elif self.max_features == \"sqrt\":\n",
        "            self.max_features_ = max(1, int(np.sqrt(n_features)))\n",
        "        elif self.max_features == \"log2\":\n",
        "            self.max_features_ = max(1, int(np.log2(n_features)))\n",
        "        else:\n",
        "            self.max_features_ = n_features\n",
        "\n",
        "        self.estimators_ = []\n",
        "        for i in range(self.n_estimators):\n",
        "            tree = DecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                random_state=self.random_state_.randint(0, 100000),\n",
        "                criterion=self.criterion,\n",
        "                splitter='random',\n",
        "                max_features=self.max_features_\n",
        "            )\n",
        "\n",
        "            if self.bootstrap:\n",
        "                # Optional bootstrapping (not default for Extra Trees)\n",
        "                X_sample, y_sample = resample(X, y,\n",
        "                    random_state=self.random_state_.randint(0, 100000))\n",
        "            else:\n",
        "                # Use full dataset (default for Extra Trees)\n",
        "                X_sample, y_sample = X, y\n",
        "\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.estimators_.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Check if model has been fitted\n",
        "        check_is_fitted(self)\n",
        "        X = check_array(X)\n",
        "\n",
        "        # Get predictions from all trees and average them\n",
        "        predictions = np.array([tree.predict(X) for tree in self.estimators_])\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Returns the coefficient of determination R^2 of the prediction.\"\"\"\n",
        "        from sklearn.metrics import r2_score\n",
        "        return r2_score(y, self.predict(X))"
      ],
      "metadata": {
        "id": "UOehsepmPToO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "import numpy as np\n",
        "custom_ex_rf_regressor = CustomExtraTreesRegressor(n_estimators = 100, random_state = 0, criterion='poisson')\n",
        "custom_ex_rf_regressor.fit(X_train, y_train)\n",
        "y_pred = custom_ex_rf_regressor.predict(X_test)\n",
        "print(\"Absolute error is\", np.max(np.abs(y_test - y_pred)), \", Relative error is\", np.max(np.abs((y_test - y_pred)/y_test)))\n",
        "print(\"Mean absolute error is\", np.mean(np.abs(y_test - y_pred)), \", Mean relative error is\", np.mean(np.abs((y_test - y_pred)/y_test)))\n",
        "print(\"Median absolute error is\", np.median(np.abs(y_test - y_pred)), \", Median relative error is\", np.median(np.abs((y_test - y_pred)/y_test)))\n",
        "\n",
        "result = permutation_importance(custom_ex_rf_regressor, X_test, y_test, n_repeats=100, random_state=0)\n",
        "importances = result.importances_mean\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Permutation Importance Feature ranking:\")\n",
        "for f in range(X_train.shape[1]):\n",
        "    print(\"\\t%d. feature %s (%f)\" % (f + 1, feature_names[indices[f]], importances[indices[f]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfNAbWOmPUay",
        "outputId": "4dafd7ce-c9aa-4fb1-9d0e-c2e0829e5e15"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Absolute error is 1.2149999999999928 , Relative error is 0.04836783439490417\n",
            "Mean absolute error is 0.03525911635593807 , Mean relative error is 0.0014201725741706242\n",
            "Median absolute error is 6.750155989720952e-14 , Median relative error is 2.6998718167013193e-15\n",
            "Permutation Importance Feature ranking:\n",
            "\t1. feature air_humidity (0.646472)\n",
            "\t2. feature air_temperature (0.487404)\n",
            "\t3. feature depth (0.333969)\n",
            "\t4. feature air_pressure (0.267929)\n",
            "\t5. feature solar_DNI (0.168632)\n",
            "\t6. feature direction (0.099341)\n",
            "\t7. feature height (0.080147)\n",
            "\t8. feature wind_speed (0.072866)\n",
            "\t9. feature wind_direction (0.049609)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBnDhbMq6qEc"
      },
      "outputs": [],
      "source": [
        "# prompt: plot with a corilation matrixreplace temp as a feature and use it as the thing to take the derivate over, add time to the feature list\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Plot the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsQgtMOO8723"
      },
      "outputs": [],
      "source": [
        "# prompt: graph the above in a matrix of scatter plots that share an y axis of temp\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the cleaned data\n",
        "data = pd.read_csv('./data.csv', index_col='date_time')\n",
        "\n",
        "# Create the scatter plot matrix\n",
        "sns.pairplot(data, diag_kind='kde', hue='temperature')  # Use temperature as hue\n",
        "plt.suptitle('Scatterplot Matrix with Shared Y-axis (Temperature)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gljjnyerleMJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV, cross_val_score\n",
        "\n",
        "model = ExtraTreesRegressor(random_state=42,\n",
        "                            n_estimators=50,\n",
        "                            max_depth=20,\n",
        "                            min_samples_leaf=4,\n",
        "                            min_samples_split=2)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "print(\"Test R^2:\", test_r2)\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], align='center')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance from Random Forest')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Display the sorted feature importance\n",
        "print(feature_importance_df)\n",
        "num_bins = 30\n",
        "hist_original, bin_edges = np.histogram(preds_original, bins=num_bins, density=True)\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# 4. Permutation-based feature importance with KL Divergence\n",
        "#----------------------------------------------------------------------\n",
        "importances = []\n",
        "\n",
        "X_test_copy = X_test.copy()  # to avoid modifying the original test set\n",
        "\n",
        "for col_idx in range(X_test.shape[1]):\n",
        "    # Shuffle the values of feature col_idx\n",
        "    X_test_copy[:, col_idx] = np.random.permutation(X_test_copy[:, col_idx])\n",
        "\n",
        "    # Predict with the permuted feature\n",
        "    preds_permuted = model.predict(X_test_copy)\n",
        "\n",
        "    # Compute the new histogram\n",
        "    hist_permuted, _ = np.histogram(preds_permuted, bins=bin_edges, density=True)\n",
        "\n",
        "    # Compute KL divergence: D_KL( original || permuted )\n",
        "    # Using scipy.stats.entropy(p, q)\n",
        "    # (Make sure neither p nor q are zero anywhere; otherwise KL can blow up to infinity)\n",
        "    # We'll add a tiny epsilon to avoid zero bins:\n",
        "    epsilon = 1e-9\n",
        "    p = hist_original + epsilon\n",
        "    q = hist_permuted + epsilon\n",
        "\n",
        "    kl_div = entropy(p, q)\n",
        "\n",
        "    importances.append(kl_div)\n",
        "\n",
        "    # Undo permutation for next feature so each feature test is isolated\n",
        "    X_test_copy[:, col_idx] = X_test[:, col_idx]\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# 5. Create a DataFrame of feature importances based on KL divergence\n",
        "#----------------------------------------------------------------------\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"KL Divergence\": importances\n",
        "}).sort_values(\"KL Divergence\", ascending=False)\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# 6. Plot the feature importances\n",
        "#----------------------------------------------------------------------\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"KL Divergence\"], align='center')\n",
        "plt.xlabel(\"KL Divergence\")\n",
        "plt.title(\"Permutation-Based Feature Importance via KL Divergence\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Print the sorted DataFrame\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktsy-GzzDREQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "model = ExtraTreesRegressor(\n",
        "    random_state=42,\n",
        "    n_estimators=50,\n",
        "    max_depth=20,\n",
        "    min_samples_leaf=4,\n",
        "    min_samples_split=2\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# (4) Get predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# (5) Evaluate standard regression metrics (optional)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2  = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"R^2:\", r2)\n",
        "\n",
        "# (6) Estimate distributions for y_test and y_pred\n",
        "#     For continuous variables, we often approximate with histograms or kernel density estimates\n",
        "num_bins = 50\n",
        "\n",
        "# True distribution\n",
        "hist_true, bin_edges = np.histogram(y_test, bins=num_bins, density=True)\n",
        "# Predicted distribution (use the same bin edges for an apples-to-apples comparison)\n",
        "hist_pred, _ = np.histogram(y_pred, bins=bin_edges, density=True)\n",
        "\n",
        "# (7) Compute KL divergence\n",
        "#     D_KL( p || q ) = sum_{i} p[i] * log( p[i] / q[i] )\n",
        "#     Here, p = hist_true, q = hist_pred\n",
        "#     Using scipy.stats.entropy(p, q) to compute KL(p||q)\n",
        "kl_div = entropy(hist_true, hist_pred)\n",
        "\n",
        "print(\"KL Divergence between test and predicted distributions:\", kl_div)\n",
        "\n",
        "# (8) (Optional) Visualize distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "width = 0.4 * (bin_edges[1] - bin_edges[0])  # some smaller width for clarity\n",
        "\n",
        "# plot \"true\" distribution histogram\n",
        "plt.bar(bin_edges[:-1], hist_true, width=width, alpha=0.5, label='True Distribution')\n",
        "\n",
        "# plot \"predicted\" distribution histogram\n",
        "plt.bar(bin_edges[:-1] + width, hist_pred, width=width, alpha=0.5, label='Predicted Distribution')\n",
        "\n",
        "plt.title('Distributions of True vs Predicted Values')\n",
        "plt.xlabel('Temp')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "num_bins = 30\n",
        "hist_original, bin_edges = np.histogram(y_pred, bins=num_bins, density=True)\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# 4. Permutation-based feature importance with KL Divergence\n",
        "#----------------------------------------------------------------------\n",
        "importances = []\n",
        "\n",
        "X_test_copy = X_test.copy()  # to avoid modifying the original test set\n",
        "\n",
        "for col_idx in range(X_test.shape[1]):\n",
        "    # Shuffle the values of feature col_idx\n",
        "    X_test_copy[:, col_idx] = np.random.permutation(X_test_copy[:, col_idx])\n",
        "\n",
        "    # Predict with the permuted feature\n",
        "    preds_permuted = model.predict(X_test_copy)\n",
        "\n",
        "    # Compute the new histogram\n",
        "    hist_permuted, _ = np.histogram(preds_permuted, bins=bin_edges, density=True)\n",
        "\n",
        "    # Compute KL divergence: D_KL( original || permuted )\n",
        "    # Using scipy.stats.entropy(p, q)\n",
        "    # (Make sure neither p nor q are zero anywhere; otherwise KL can blow up to infinity)\n",
        "    # We'll add a tiny epsilon to avoid zero bins:\n",
        "    epsilon = 1e-9\n",
        "    p = hist_original + epsilon\n",
        "    q = hist_permuted + epsilon\n",
        "\n",
        "    kl_div = entropy(p, q)\n",
        "\n",
        "    importances.append(kl_div)\n",
        "\n",
        "    # Undo permutation for next feature so each feature test is isolated\n",
        "    X_test_copy[:, col_idx] = X_test[:, col_idx]\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# 5. Create a DataFrame of feature importances based on KL divergence\n",
        "#----------------------------------------------------------------------\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"KL Divergence\": importances\n",
        "}).sort_values(\"KL Divergence\", ascending=False)\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "# 6. Plot the feature importances\n",
        "#----------------------------------------------------------------------\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"KL Divergence\"], align='center')\n",
        "plt.xlabel(\"KL Divergence\")\n",
        "plt.title(\"Permutation-Based Feature Importance via KL Divergence\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Print the sorted DataFrame\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-XIyKvBnJxY"
      },
      "outputs": [],
      "source": [
        "# prompt: Compare temp, solar, and humidity over time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'combined' DataFrame from the previous code is available\n",
        "# If not, load it: combined = pd.read_csv('./data.csv', index_col='date_time')\n",
        "\n",
        "# Plotting temperature, solar radiation, and humidity over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(combined.index, combined['temperature'], label='Temperature')\n",
        "plt.plot(combined.index, combined['solar_DNI'], label='Solar DNI')\n",
        "plt.plot(combined.index, combined['air_humidity'], label='Air Humidity')\n",
        "\n",
        "\n",
        "plt.xlabel('Date and Time')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Temperature, Solar Radiation, and Humidity Over Time')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG_ANZJZpSCD"
      },
      "outputs": [],
      "source": [
        "import scipy.signal as signal\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# Calculate the derivative of features over time\n",
        "for col in ['temperature', 'solar_DNI', 'air_humidity', 'wind_speed', 'wind_direction', 'air_temperature', 'air_pressure']:\n",
        "    combined[f'{col}_derivative'] = combined[col].diff()\n",
        "\n",
        "# Feature correlation matrix\n",
        "correlation_matrix = combined.corr()\n",
        "\n",
        "# Plotting the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)\n",
        "plt.yticks(range(len(correlation_matrix.index)), correlation_matrix.index)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Displaying the correlation matrix numerically\n",
        "correlation_matrix\n",
        "\n",
        "# prompt: rank all freatures agisnt time using spectral denstiy, overlay plots, and find the best the AUC\n",
        "\n",
        "\n",
        "# Calculate spectral density for each feature\n",
        "sampling_frequency = 1\n",
        "frequencies = []\n",
        "spectral_densities = []\n",
        "\n",
        "for column in combined.columns:\n",
        "    if column != 'date_time' and combined[column].dtype != 'object':  # Exclude non-numeric columns and date_time\n",
        "        f, Pxx_den = signal.welch(combined[column], fs=sampling_frequency)\n",
        "        frequencies.append(f)\n",
        "        spectral_densities.append(Pxx_den)\n",
        "\n",
        "# Overlay plots for spectral densities\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for i, column in enumerate(combined.columns):\n",
        "    if column != 'date_time' and combined[column].dtype != 'object':\n",
        "        plt.semilogy(frequencies[i], spectral_densities[i], label=column)\n",
        "\n",
        "plt.xlabel('Frequency (cycles/hour)')\n",
        "plt.ylabel('Spectral Density')\n",
        "plt.title('Spectral Density of Features')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Example for ranking features (adjust the ranking criteria)\n",
        "# Here, we rank based on the maximum spectral density.  You can adjust\n",
        "# this based on what represents significance in your data.\n",
        "ranked_features = sorted(\n",
        "    [(col, np.max(spectral_densities[i])) for i, col in enumerate(combined.columns) if col != 'date_time' and combined[col].dtype != 'object'],\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True,\n",
        ")\n",
        "\n",
        "print(\"Ranked Features by Maximum Spectral Density:\")\n",
        "for feature, max_density in ranked_features:\n",
        "    print(f\"{feature}: {max_density}\")\n",
        "\n",
        "\n",
        "# --- AUC Calculation (Example) ---\n",
        "\n",
        "# Assuming 'temperature' is your target variable and you have a binary classification\n",
        "# problem based on some threshold of 'temperature'.\n",
        "temperature_threshold = 25 # Example threshold\n",
        "\n",
        "# Create a binary target variable.  Replace with your classification logic.\n",
        "y_true = (combined['temperature'] > temperature_threshold).astype(int)\n",
        "\n",
        "# Choose a feature for AUC Calculation\n",
        "auc_feature = 'solar_DNI' # Example feature; you can change this\n",
        "y_pred_proba = combined[auc_feature]  # Use the chosen feature as the prediction probability\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_true, y_pred_proba)\n",
        "print(f\"\\nAUC for '{auc_feature}' vs. Temperature Threshold: {auc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVDk5GHD05ix"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeVHnBEavgMJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned data\n",
        "data = pd.read_csv('./data.csv', index_col='date_time')\n",
        "\n",
        "# Define the target variable (temperature) and features\n",
        "X = data.drop(['temperature', 'space'], axis=1)  # Drop target and unnecessary columns\n",
        "y = data['temperature']\n",
        "\n",
        "# Keep a copy of the feature names before scaling\n",
        "feature_names = X.columns  # Store column names here\n",
        "\n",
        "# Scale features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "# feature_names = X.columns  # Use the original feature names\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], align='center')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance from Random Forest')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Display the sorted feature importance\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Model Evaluation:\")\n",
        "print(f\"Score: {model.score(X_test, y_test):.2f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R-squared (R²): {r2_score(y_test, y_pred):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYfxgMYTIhuC"
      },
      "source": [
        "How It Works\n",
        "\n",
        "    divergence_fn\n",
        "        This parameter is a callable that takes (p, q)—two arrays representing probability distributions (e.g., histograms)—and returns a float.\n",
        "        You can implement KL divergence, Jensen–Shannon divergence, Hellinger distance, or even a custom measure.\n",
        "\n",
        "    Permutation Step\n",
        "        For each feature, we shuffle its values across all test samples, making that feature “nonsensical” to the model for those predictions.\n",
        "        We measure how much the predicted distribution changes relative to the baseline (non-shuffled) predictions.\n",
        "\n",
        "    Importance Ranking\n",
        "        Features that produce the largest divergence (i.e., cause the biggest shift in the prediction distribution) are deemed more “important.”\n",
        "\n",
        "    Adjusting Bin Size or Using KDE\n",
        "        The num_bins can be adjusted, or you can replace the histogram approach with a kernel density estimate if you prefer a smoother distribution approximation.\n",
        "        For KDE, you might use scipy.stats.gaussian_kde or sklearn.neighbors.KernelDensity to get p(x) and q(x) on a grid, then compute the divergence.\n",
        "\n",
        "Key Takeaways\n",
        "\n",
        "    Flexible Divergence: You can plug in any divergence function—this snippet is not tied to KL specifically.\n",
        "    Permutation Logic: The approach is the same as standard permutation importance, but instead of measuring change in MSE (or R²), we measure change in distribution (via your chosen divergence function).\n",
        "    Interpretation: A higher divergence indicates a feature whose permutation drastically alters the shape of the prediction distribution—potentially indicating it’s a critical feature for the model’s overall output distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4b_bwkmGoCP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from scipy.stats import differential_entropy\n",
        "\n",
        "def custom_divergence(p, q, epsilon=1e-9):\n",
        "    p_ = p + epsilon\n",
        "    q_ = q + epsilon\n",
        "    return entropy(p_, q_)\n",
        "\n",
        "def permutation_importance_distribution(model, X, num_bins=100, random_state=42):\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    preds = model.predict(X)\n",
        "    hist_orig, bin_edges = np.histogram(preds, density=True)\n",
        "    importances = []\n",
        "    X_copy = X.copy()\n",
        "    for i in range(X.shape[1]):\n",
        "        shuffled = rng.permutation(X_copy[:, i])\n",
        "        X_copy[:, i] = shuffled\n",
        "        preds_shuffled = model.predict(X_copy)\n",
        "        hist_shuffled, _ = np.histogram(preds_shuffled, bins=bin_edges, density=True)\n",
        "        importances.append(custom_divergence(hist_orig, hist_shuffled))\n",
        "        X_copy[:, i] = X[:, i]\n",
        "    return importances\n",
        "\n",
        "data = data.sample(frac=1, random_state=42)\n",
        "print(len(data))\n",
        "print(len(data))\n",
        "X = data.drop(['temperature'], axis=1)\n",
        "y = data['temperature']\n",
        "feature_names = X.columns\n",
        "scaler = RobustScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = ExtraTreesRegressor(random_state=42, n_estimators=75, max_depth=30, min_samples_leaf=4, min_samples_split=2)\n",
        "model.fit(X_train, y_train)\n",
        "importances = permutation_importance_distribution(model, X_test)\n",
        "df = pd.DataFrame({'Feature': feature_names, 'Divergence': importances}).sort_values('Divergence', ascending=False)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgn5nl27KZIr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import differential_entropy\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "def custom_divergence(samples):\n",
        "    return differential_entropy(samples, base=np.e, method='vasicek')\n",
        "\n",
        "def permutation_based_importance(model, X, divergence_fn, random_state=42):\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    baseline_preds = model.predict(X)\n",
        "    baseline_div = divergence_fn(baseline_preds)\n",
        "    importances = []\n",
        "    X_copy = X.copy()\n",
        "    for i in range(X.shape[1]):\n",
        "        shuffled = rng.permutation(X_copy[:, i])\n",
        "        X_copy[:, i] = shuffled\n",
        "        perm_preds = model.predict(X_copy)\n",
        "        perm_div = divergence_fn(perm_preds)\n",
        "        importances.append(abs(baseline_div - perm_div))\n",
        "        X_copy[:, i] = X[:, i]\n",
        "    return importances\n",
        "\n",
        "data = data.sample(frac=1, random_state=42)\n",
        "X = data.drop(['temperature'], axis=1)\n",
        "y = data['temperature']\n",
        "feature_names = X.columns\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "model = ExtraTreesRegressor(random_state=42, n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "importances = permutation_based_importance(model, X_test, custom_divergence)\n",
        "df = pd.DataFrame({'Feature': feature_names, 'Divergence': importances}).sort_values('Divergence', ascending=False)\n",
        "print(df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}